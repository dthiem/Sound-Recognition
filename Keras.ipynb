{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from matplotlib.pyplot import specgram\n",
    "import math\n",
    "from random import shuffle\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "plt.rcParams['figure.titlesize'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def appendSounds(data,labels,fp):\n",
    "    X, sr = sf.read(fp)\n",
    "    sound = np.array(X)\n",
    "    # librosa operates on (lenght, channels) matrices, wheras soundfile gave us (channels, lenght) \n",
    "    # so we transpose\n",
    "    sound = np.transpose(sound)\n",
    "    sound = librosa.core.to_mono(sound)\n",
    "    # resample so every wave has same sampling rate\n",
    "    sound = librosa.core.resample(sound, sr, 10000)\n",
    "    # set class number\n",
    "    classNumber = int(fp.split(\"/\")[1].split(\"-\")[1])\n",
    "    # compute and set mel spectrogram\n",
    "    mel = librosa.feature.melspectrogram(sound, sr=10000,n_mels=60, hop_length=506)\n",
    "    iterator, sliceSize = 0,10\n",
    "    while iterator+10<=mel.shape[1]:\n",
    "        data.append(mel[:,iterator:iterator+10].flatten())\n",
    "        labels.append(classNumber)\n",
    "        iterator+=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm done, time was: 649.290421963\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Lock, Pipe,Event\n",
    "import time\n",
    "from keras import utils\n",
    "\n",
    "\n",
    "\n",
    "trainData,trainLabels= [],[]\n",
    "tuneData,tuneLabels=[],[]\n",
    "testData,testLabels=[],[]\n",
    "def add(x,c):\n",
    "    tempdata,templabels = [],[]\n",
    "    for file in glob.glob(x):\n",
    "        appendSounds(tempdata, templabels, file)\n",
    "    c.send(zip(tempdata, templabels))\n",
    "    del tempdata,templabels\n",
    "        \n",
    "ts = time.time()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    threads= []\n",
    "    connections=[0]*10\n",
    "    lock = Lock()\n",
    "    for x in xrange(1,11):\n",
    "        connections[x-1], childPipe=Pipe()\n",
    "        threads.append(Process(target=add, args=(\"fold\"+str(x)+\"/*.wav\",childPipe)))         \n",
    "        threads[x-1].start()\n",
    "    fold = 1\n",
    "    for x,y in zip(threads,connections):\n",
    "        tD, tL = zip(*y.recv())\n",
    "        if fold <= 8:\n",
    "            trainData += tD\n",
    "            trainLabels += tL\n",
    "#         elif fold == 8 :\n",
    "#             tuneData += tD\n",
    "#             tuneLabels += tL\n",
    "        else:\n",
    "            testData += tD\n",
    "            testLabels += tL\n",
    "        fold+=1\n",
    "    \n",
    "    trainData, trainLabels = np.array(trainData), np.array(utils.to_categorical(trainLabels, num_classes=10))\n",
    "    tuneData, tuneLabels = np.array(tuneData), np.array(utils.to_categorical(tuneLabels, num_classes=10))\n",
    "    testData, testLabels = np.array(testData), np.array(utils.to_categorical(testLabels, num_classes=10))\n",
    "    \n",
    "    print \"I'm done, time was:\" , time.time()-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "55s - loss: 2.4641 - acc: 0.4174\n",
      "Epoch 2/100\n",
      "49s - loss: 1.8108 - acc: 0.5266\n",
      "Epoch 3/100\n",
      "49s - loss: 1.6093 - acc: 0.5665\n",
      "Epoch 4/100\n",
      "49s - loss: 1.4734 - acc: 0.5972\n",
      "Epoch 5/100\n",
      "49s - loss: 1.3860 - acc: 0.6163\n",
      "Epoch 6/100\n",
      "49s - loss: 1.3199 - acc: 0.6310\n",
      "Epoch 7/100\n",
      "50s - loss: 1.2561 - acc: 0.6415\n",
      "Epoch 8/100\n",
      "49s - loss: 1.2178 - acc: 0.6501\n",
      "Epoch 9/100\n",
      "49s - loss: 1.1709 - acc: 0.6598\n",
      "Epoch 10/100\n",
      "49s - loss: 1.1654 - acc: 0.6624\n",
      "Epoch 11/100\n",
      "49s - loss: 1.1324 - acc: 0.6688\n",
      "Epoch 12/100\n",
      "49s - loss: 1.1141 - acc: 0.6729\n",
      "Epoch 13/100\n",
      "49s - loss: 1.0580 - acc: 0.6866\n",
      "Epoch 14/100\n",
      "49s - loss: 1.0504 - acc: 0.6881\n",
      "Epoch 15/100\n",
      "49s - loss: 1.0505 - acc: 0.6875\n",
      "Epoch 16/100\n",
      "49s - loss: 0.9988 - acc: 0.7009\n",
      "Epoch 17/100\n",
      "49s - loss: 0.9871 - acc: 0.7041\n",
      "Epoch 18/100\n",
      "48s - loss: 0.9789 - acc: 0.7064\n",
      "Epoch 19/100\n",
      "49s - loss: 0.9519 - acc: 0.7137\n",
      "Epoch 20/100\n",
      "49s - loss: 0.9248 - acc: 0.7199\n",
      "Epoch 21/100\n",
      "49s - loss: 0.9289 - acc: 0.7197\n",
      "Epoch 22/100\n",
      "49s - loss: 0.9797 - acc: 0.7140\n",
      "Epoch 23/100\n",
      "49s - loss: 0.9173 - acc: 0.7255\n",
      "Epoch 24/100\n",
      "49s - loss: 0.8901 - acc: 0.7317\n",
      "Epoch 25/100\n",
      "49s - loss: 0.8695 - acc: 0.7383\n",
      "Epoch 26/100\n",
      "49s - loss: 0.8475 - acc: 0.7434\n",
      "Epoch 27/100\n",
      "49s - loss: 0.8411 - acc: 0.7449\n",
      "Epoch 28/100\n",
      "49s - loss: 0.8327 - acc: 0.7502\n",
      "Epoch 29/100\n",
      "49s - loss: 0.8787 - acc: 0.7423\n",
      "Epoch 30/100\n",
      "49s - loss: 0.9032 - acc: 0.7388\n",
      "Epoch 31/100\n",
      "49s - loss: 0.8464 - acc: 0.7475\n",
      "Epoch 32/100\n",
      "49s - loss: 0.8135 - acc: 0.7555\n",
      "Epoch 33/100\n",
      "49s - loss: 0.7903 - acc: 0.7621\n",
      "Epoch 34/100\n",
      "49s - loss: 0.7770 - acc: 0.7656\n",
      "Epoch 35/100\n",
      "49s - loss: 0.7524 - acc: 0.7736\n",
      "Epoch 36/100\n",
      "49s - loss: 0.7346 - acc: 0.7799\n",
      "Epoch 37/100\n",
      "49s - loss: 0.7275 - acc: 0.7837\n",
      "Epoch 38/100\n",
      "49s - loss: 0.7282 - acc: 0.7840\n",
      "Epoch 39/100\n",
      "49s - loss: 0.7317 - acc: 0.7840\n",
      "Epoch 40/100\n",
      "49s - loss: 0.7040 - acc: 0.7927\n",
      "Epoch 41/100\n",
      "49s - loss: 0.6955 - acc: 0.7962\n",
      "Epoch 42/100\n",
      "49s - loss: 0.7127 - acc: 0.7945\n",
      "Epoch 43/100\n",
      "49s - loss: 0.7531 - acc: 0.7857\n",
      "Epoch 44/100\n",
      "49s - loss: 0.7185 - acc: 0.7931\n",
      "Epoch 45/100\n",
      "49s - loss: 0.6917 - acc: 0.7995\n",
      "Epoch 46/100\n",
      "49s - loss: 0.6832 - acc: 0.8033\n",
      "Epoch 47/100\n",
      "49s - loss: 0.6761 - acc: 0.8053\n",
      "Epoch 48/100\n",
      "49s - loss: 0.6788 - acc: 0.8080\n",
      "Epoch 49/100\n",
      "49s - loss: 0.6609 - acc: 0.8102\n",
      "Epoch 50/100\n",
      "49s - loss: 0.6418 - acc: 0.8191\n",
      "Epoch 51/100\n",
      "49s - loss: 0.7698 - acc: 0.8112\n",
      "Epoch 52/100\n",
      "48s - loss: 0.9523 - acc: 0.7654\n",
      "Epoch 53/100\n",
      "49s - loss: 0.7397 - acc: 0.7946\n",
      "Epoch 54/100\n",
      "49s - loss: 0.7131 - acc: 0.8031\n",
      "Epoch 55/100\n",
      "49s - loss: 0.6981 - acc: 0.8107\n",
      "Epoch 56/100\n",
      "49s - loss: 0.7611 - acc: 0.8003\n",
      "Epoch 57/100\n",
      "49s - loss: 0.6505 - acc: 0.8212\n",
      "Epoch 58/100\n",
      "49s - loss: 0.6289 - acc: 0.8278\n",
      "Epoch 59/100\n",
      "49s - loss: 0.6780 - acc: 0.8229\n",
      "Epoch 60/100\n",
      "49s - loss: 0.6863 - acc: 0.8210\n",
      "Epoch 61/100\n",
      "49s - loss: 0.6484 - acc: 0.8264\n",
      "Epoch 62/100\n",
      "49s - loss: 0.6072 - acc: 0.8336\n",
      "Epoch 63/100\n",
      "49s - loss: 0.5892 - acc: 0.8395\n",
      "Epoch 64/100\n",
      "48s - loss: 0.6215 - acc: 0.8341\n",
      "Epoch 65/100\n",
      "48s - loss: 0.5923 - acc: 0.8402\n",
      "Epoch 66/100\n",
      "48s - loss: 0.5787 - acc: 0.8436\n",
      "Epoch 67/100\n",
      "48s - loss: 0.5639 - acc: 0.8469\n",
      "Epoch 68/100\n",
      "48s - loss: 0.6121 - acc: 0.8363\n",
      "Epoch 69/100\n",
      "48s - loss: 0.5656 - acc: 0.8459\n",
      "Epoch 70/100\n",
      "48s - loss: 0.5843 - acc: 0.8413\n",
      "Epoch 71/100\n",
      "48s - loss: 0.5961 - acc: 0.8412\n",
      "Epoch 72/100\n",
      "48s - loss: 0.5617 - acc: 0.8478\n",
      "Epoch 73/100\n",
      "48s - loss: 0.5762 - acc: 0.8477\n",
      "Epoch 74/100\n",
      "48s - loss: 0.6990 - acc: 0.8242\n",
      "Epoch 75/100\n",
      "48s - loss: 0.5992 - acc: 0.8425\n",
      "Epoch 76/100\n",
      "48s - loss: 0.5589 - acc: 0.8492\n",
      "Epoch 77/100\n",
      "48s - loss: 0.5371 - acc: 0.8551\n",
      "Epoch 78/100\n",
      "48s - loss: 0.6140 - acc: 0.8418\n",
      "Epoch 79/100\n",
      "48s - loss: 0.6453 - acc: 0.8395\n",
      "Epoch 80/100\n",
      "48s - loss: 0.5480 - acc: 0.8534\n",
      "Epoch 81/100\n",
      "48s - loss: 0.5704 - acc: 0.8507\n",
      "Epoch 82/100\n",
      "48s - loss: 0.5574 - acc: 0.8509\n",
      "Epoch 83/100\n",
      "48s - loss: 0.5796 - acc: 0.8481\n",
      "Epoch 84/100\n",
      "48s - loss: 0.5148 - acc: 0.8601\n",
      "Epoch 85/100\n",
      "48s - loss: 0.5052 - acc: 0.8630\n",
      "Epoch 86/100\n",
      "47s - loss: 0.5625 - acc: 0.8514\n",
      "Epoch 87/100\n",
      "47s - loss: 0.5031 - acc: 0.8618\n",
      "Epoch 88/100\n",
      "48s - loss: 0.5732 - acc: 0.8522\n",
      "Epoch 89/100\n",
      "48s - loss: 0.4992 - acc: 0.8640\n",
      "Epoch 90/100\n",
      "48s - loss: 0.4763 - acc: 0.8704\n",
      "Epoch 91/100\n",
      "48s - loss: 0.4654 - acc: 0.8750\n",
      "Epoch 92/100\n",
      "48s - loss: 0.5065 - acc: 0.8662\n",
      "Epoch 93/100\n",
      "48s - loss: 0.4895 - acc: 0.8707\n",
      "Epoch 94/100\n",
      "47s - loss: 0.4867 - acc: 0.8696\n",
      "Epoch 95/100\n",
      "47s - loss: 0.4646 - acc: 0.8758\n",
      "Epoch 96/100\n",
      "47s - loss: 0.6197 - acc: 0.8567\n",
      "Epoch 97/100\n",
      "47s - loss: 0.5276 - acc: 0.8615\n",
      "Epoch 98/100\n",
      "47s - loss: 0.4809 - acc: 0.8725\n",
      "Epoch 99/100\n",
      "47s - loss: 0.4973 - acc: 0.8697\n",
      "Epoch 100/100\n",
      "47s - loss: 0.4935 - acc: 0.8701\n",
      "11856/11914 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[2.9540012826450406, 0.53063622626838858]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "net = Sequential()\n",
    "net.add(Dense(1000, activation='relu', input_dim=1280))\n",
    "net.add(Dense(600, activation='relu'))\n",
    "net.add(Dense(170, activation='relu'))\n",
    "net.add(Dense(10, activation='softmax'))\n",
    "\n",
    "net.compile(loss='categorical_crossentropy',\n",
    "           optimizer = SGD(lr=0.0003),\n",
    "           metrics = ['accuracy'])\n",
    "net.fit(trainData,trainLabels, epochs = 100, batch_size=16, verbose=2)\n",
    "score=net.evaluate(testData,testLabels, batch_size=16)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0773913470494461, 0.4402383750109779]\n"
     ]
    }
   ],
   "source": [
    "print score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
