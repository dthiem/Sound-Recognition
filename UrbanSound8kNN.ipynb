{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from matplotlib.pyplot import specgram\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from random import shuffle\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "plt.rcParams['figure.titlesize'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def appendSounds(arrayToAppend,fp):\n",
    "    X, sr = sf.read(fp)\n",
    "    sound = np.array(X)\n",
    "    # librosa operates on (lenght, channels) matrices, wheras soundfile gave us (channels, lenght) \n",
    "    # so we transpose\n",
    "    sound = np.transpose(sound)\n",
    "    sound = librosa.core.to_mono(sound)\n",
    "    # resample so every wave has same sampling rate\n",
    "    sound = librosa.core.resample(sound, sr, 10000)\n",
    "    # set class number\n",
    "    classNumber = int(fp.split(\"/\")[1].split(\"-\")[1])\n",
    "    # compute and set mel spectrogram\n",
    "    mel = librosa.feature.melspectrogram(sound, sr=10000, hop_length=506, n_mels = 64)\n",
    "    iterator, sliceSize = 0,10\n",
    "    while iterator+10<=mel.shape[1]:\n",
    "        arrayToAppend.append(Sound(classNumber,mel[:,iterator:iterator+10]))\n",
    "        iterator+=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sound:\n",
    "    def __init__(self, classNumber, mel):\n",
    "        self.classNumber=classNumber\n",
    "        self.mel = mel\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 (64, 10)\n",
      "4 (64, 10)\n",
      "4 (64, 10)\n",
      "4 (64, 10)\n",
      "4 (64, 10)\n",
      "4 (64, 10)\n",
      "4 (64, 10)\n",
      "4 (64, 10)\n"
     ]
    }
   ],
   "source": [
    "ayy = []\n",
    "appendSounds(ayy, \"fold1/14113-4-0-1.wav\")\n",
    "for x in ayy:\n",
    "    print x.classNumber, x.mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    countNetwoks=0\n",
    "    def __init__(self,numin=None,numout=None,hiddenlayers=None,learningrate=None,arrtrain=None,arrtest=None,neural=None):\n",
    "        NeuralNetwork.countNetwoks+=1\n",
    "        self.myId=NeuralNetwork.countNetwoks\n",
    "        if neural!=None:\n",
    "            self.numIn=neural.numIn\n",
    "            self.numOut=neural.numOut\n",
    "            self.hiddenLayers=neural.hiddenLayers[:]\n",
    "            self.learningRate=neural.learningRate\n",
    "            if numin!=None:\n",
    "                self.numIn = numin\n",
    "            if numout!= None:\n",
    "                self.numOut= numout\n",
    "            if hiddenlayers!= None:\n",
    "                self.hiddenLayers=hiddenlayers[:]\n",
    "            if learningrate!= None:\n",
    "                self.learningRate=learningrate\n",
    "        else:\n",
    "            self.numIn=numin\n",
    "            self.numOut=numout\n",
    "            self.hiddenLayers = hiddenlayers\n",
    "            self.learningRate = learningrate\n",
    "    def train(self,arrtrain,arrtest=None):\n",
    "        prepareNet(self.numIn,self.numOut,self.hiddenLayers,self.learningRate,arrtrain,self.myId)\n",
    "        if arrtest!= None:\n",
    "            self.scores = testNet(arrtest, self.myId)\n",
    "            self.mainScore = self.scores.trace()/self.scores.sum()\n",
    "    def test(self,arrtest):\n",
    "            self.scores = testNet(arrtest, self.myId)\n",
    "            self.mainScore = self.scores.trace()/self.scores.sum()        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm done, time was: 1747.25513101\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Lock, Pipe,Event\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "arrtrain = []\n",
    "arrtuning=[]\n",
    "arrtest=[]\n",
    "def add(x,c):\n",
    "    temp = []\n",
    "    for file in glob.glob(x):\n",
    "        appendSounds(temp,file)\n",
    "    c.send(temp)\n",
    "    del temp\n",
    "        \n",
    "ts = time.time()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    threads= []\n",
    "    connections=[0]*10\n",
    "    lock = Lock()\n",
    "    for x in xrange(1,11):\n",
    "        connections[x-1],childPipe=Pipe()\n",
    "        threads.append(Process(target=add, args=(\"fold\"+str(x)+\"/*.wav\",childPipe)))         \n",
    "        threads[x-1].start()\n",
    "    fold = 1\n",
    "    for x,y in zip(threads,connections):\n",
    "        if fold < 9:\n",
    "           arrtrain.extend(y.recv())\n",
    "        elif fold == 8 :\n",
    "           arrtuning.extend(y.recv())\n",
    "        else:\n",
    "           arrtest.extend(y.recv())\n",
    "        fold+=1\n",
    "    print \"I'm done, time was:\" , time.time()-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 2.4 Build inference graph.\n",
    "def mnist_inference(sounds, numin,numout,hiddenLayers):\n",
    "    \"\"\"Build the MNIST model up to where it may be used for inference.\n",
    "    Args:\n",
    "        images: Images placeholder.\n",
    "        hidden1_units: Size of the first hidden layer.\n",
    "        hidden2_units: Size of the second hidden layer.\n",
    "    Returns:\n",
    "        logits: Output tensor with the computed logits.\n",
    "    \"\"\"\n",
    "    hidden=[]\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([numin, hiddenLayers[0]],\n",
    "                                stddev=1.0 / math.sqrt(float(numin))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hiddenLayers[0]]),\n",
    "                             name='biases')\n",
    "        hidden.append(tf.nn.relu(tf.matmul(sounds, weights) + biases))\n",
    "    \n",
    "    for i in xrange(1,len(hiddenLayers)):\n",
    "        with tf.name_scope('hidden'+str(i+1)):\n",
    "            weights = tf.Variable(\n",
    "                tf.truncated_normal([hiddenLayers[i-1], hiddenLayers[i]],\n",
    "                                    stddev=1.0 / math.sqrt(float(hiddenLayers[i-1]))),\n",
    "                name='weights')\n",
    "            biases = tf.Variable(tf.zeros([hiddenLayers[i]]),\n",
    "                                 name='biases')\n",
    "            hidden.append(tf.nn.relu(tf.matmul(hidden[i-1], weights) + biases))\n",
    "        \n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hiddenLayers[-1], numout],\n",
    "                                stddev=1.0 / math.sqrt(float(hiddenLayers[-1]))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([numout]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden[-1], weights) + biases\n",
    "\n",
    "    # Uncomment the following line to see what we have constructed.\n",
    "    # tf.train.write_graph(tf.get_default_graph().as_graph_def(),\n",
    "    #                      \"/tmp\", \"inference.pbtxt\", as_text=True)\n",
    "    return logits\n",
    "\n",
    "# 2.5 Build training graph.\n",
    "def mnist_training(logits, labels, learning_rate):\n",
    "    \"\"\"Build the training graph.\n",
    "\n",
    "    Args:\n",
    "        logits: Logits tensor, float - [BATCH_SIZE, NUM_CLASSES].\n",
    "        labels: Labels tensor, int32 - [BATCH_SIZE], with values in the\n",
    "          range [0, NUM_CLASSES).\n",
    "        learning_rate: The learning rate to use for gradient descent.\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "        loss: The Op for calculating loss.\n",
    "    \"\"\"\n",
    "    # Create an operation that calculates loss.\n",
    "    labels = tf.to_int64(labels)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits, labels, name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Uncomment the following line to see what we have constructed.\n",
    "    # tf.train.write_graph(tf.get_default_graph().as_graph_def(),\n",
    "    #                      \"/tmp\", \"train.pbtxt\", as_text=True)\n",
    "\n",
    "    return train_op, loss\n",
    "\n",
    "# 2.6 Build the complete graph for feeding inputs, training, and saving checkpoints.\n",
    "def prepareNet(numin,numout,hiddenLayers, learningRate,trainset,saveId):\n",
    "    mnist_graph = tf.Graph()\n",
    "    with mnist_graph.as_default():\n",
    "        # Generate placeholders for the images and labels.\n",
    "        images_placeholder = tf.placeholder(tf.float32)                                       \n",
    "        labels_placeholder = tf.placeholder(tf.int32)\n",
    "        tf.add_to_collection(\"images\", images_placeholder)  # Remember this Op.\n",
    "        tf.add_to_collection(\"labels\", labels_placeholder)  # Remember this Op.\n",
    "\n",
    "        # Build a Graph that computes predictions from the inference model.\n",
    "        logits = mnist_inference(images_placeholder,numin,numout,hiddenLayers)\n",
    "        tf.add_to_collection(\"logits\", logits)  # Remember this Op.\n",
    "\n",
    "        # Add to the Graph the Ops that calculate and apply gradients.\n",
    "        train_op, loss = mnist_training(logits, labels_placeholder, learningRate)\n",
    "\n",
    "        # Add the variable initializer Op.\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Create a saver for writing training checkpoints.\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Uncomment the following line to see what we have constructed.\n",
    "        # tf.train.write_graph(tf.get_default_graph().as_graph_def(),\n",
    "        #                      \"/tmp\", \"complete.pbtxt\", as_text=True)\n",
    "    \n",
    "    # 2.7 Run training for MAX_STEPS and save checkpoint at the end.\n",
    "    with tf.Session(graph=mnist_graph) as sess:\n",
    "        # Run the Op to initialize the variables.\n",
    "        sess.run(init)\n",
    "        losses = []\n",
    "        counter=0\n",
    "        # Start the training loop.\n",
    "        for i in xrange (10):\n",
    "            shuffle(trainset)\n",
    "            iterator = 0\n",
    "            # Read a batch of images and labels.\n",
    "            #images_feed, labels_feed = data_sets.train.next_batch(BATCH_SIZE)\n",
    "            print \"Step:\", i+1\n",
    "            while iterator<=len(trainset):\n",
    "                mel_feed, labels_feed = [],[]\n",
    "                for x in trainset[iterator:iterator+16]:\n",
    "                    mel_feed.append(x.mel.flatten())\n",
    "                    labels_feed.append(x.classNumber)\n",
    "                #print np.array(mel_feed).shape, \" \", np.array(labels_feed).shape\n",
    "\n",
    "\n",
    "                # Run one step of the model.  The return values are the activations\n",
    "                # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "                # inspect the values of your Ops or variables, you may include them\n",
    "                # in the list passed to sess.run() and the value tensors will be\n",
    "                # returned in the tuple from the call.\n",
    "                if not (len(labels_feed)==0):\n",
    "                    _, loss_value = sess.run([train_op, loss],\n",
    "                                             feed_dict={images_placeholder: np.array(mel_feed),\n",
    "                                                        labels_placeholder: np.array(labels_feed)})\n",
    "                    losses.append(loss_value)\n",
    "                # Print out loss value.\n",
    "                counter +=1\n",
    "                #if counter % 1000 == 0:\n",
    "                    #print('Step %d: loss = %.2f' % (counter, loss_value))\n",
    "                iterator+=16\n",
    "\n",
    "\n",
    "\n",
    "        # Write a checkpoint.\n",
    "        #checkpoint_file = os.path.join(TRAIN_DIR, 'checkpoint')\n",
    "        #saver.save(sess, checkpoint_file, global_step=step)\n",
    "        checkpoint_file = os.path.join(\"/tmp/sound\", 'checkpoint')\n",
    "        saver.save(sess, checkpoint_file,global_step=saveId)\n",
    "        plt.plot(losses)\n",
    "\n",
    "def testNet(testingset,saveId):\n",
    "\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            os.path.join(\"/tmp/sound\", \"checkpoint-\"+str(saveId)+\".meta\"))\n",
    "        saver.restore(\n",
    "            sess, os.path.join(\"/tmp/sound\", \"checkpoint-\"+str(saveId)))\n",
    "\n",
    "\n",
    "        # Retrieve the Ops we 'remembered'.\n",
    "        logits = tf.get_collection(\"logits\")[0]\n",
    "        images_placeholder = tf.get_collection(\"images\")[0]\n",
    "        labels_placeholder = tf.get_collection(\"labels\")[0]\n",
    "        # Add an Op that chooses the top k predictions.\n",
    "\n",
    "        eval_op = tf.nn.top_k(logits)\n",
    "\n",
    "        scores = np.zeros((10,10))\n",
    "        for x in testingset:\n",
    "            mel_feed=[x.mel.flatten()]\n",
    "            labels_feed=[x.classNumber]\n",
    "            mel_feed, labels_feed = np.array(mel_feed), np.array(labels_feed)\n",
    "\n",
    "                # Run evaluation.\n",
    "                #images_feed, labels_feed = data_sets.validation.next_batch(EVAL_BATCH_SIZE)\n",
    "            prediction = sess.run(eval_op,\n",
    "                                      feed_dict={images_placeholder: mel_feed,\n",
    "                                                 labels_placeholder: labels_feed})\n",
    "                #print type(labels_feed[0]), type(prediction.indices[0][0])\n",
    "            temp=(prediction.indices[0][0])\n",
    "                #if labels_feed[0] == prediction.indices[0][0]:\n",
    "                #   good+=1\n",
    "                #total +=1\n",
    "            \n",
    "            scores[labels_feed[0],temp]+=1\n",
    "            #if good*2 >= total:\n",
    "             #   print labels_feed, \" : \", good, \"/\", total\n",
    "            #print count\n",
    "        return scores\n",
    "        #counter = 0\n",
    "        #for x in scores:\n",
    "        #    print \"Class %d: %.2f  Quantity: %d\" %(counter, x[counter]/np.sum(x), np.sum(x))\n",
    "        #    counter +=1\n",
    "        #print \"Final score: \", scores.trace()/scores.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findLearningRate(neural,learnValues,arrtrain,arrtune):\n",
    "    neuralNets=[]\n",
    "    for x in learnValues:\n",
    "        neuralNets.append(NeuralNetwork(learningrate=x, neural=neural))\n",
    "    for x in neuralNets:\n",
    "        x.train(arrtrain,arrtune)\n",
    "        print \"Learning rate:\", x.learningRate, \"score:\", x.mainScore\n",
    "    bestNet,bestScore=0,0\n",
    "    for i in xrange (len(neuralNets)):\n",
    "        if bestScore<neuralNets[i].mainScore:\n",
    "            bestScore=neuralNets[i].mainScore\n",
    "            bestNet=i\n",
    "    return neuralNets[bestNet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d= NeuralNetwork(640,10,[500,600,170])\n",
    "bestNet=findLearningRate(d,[0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.001],arrtrain,arrtuning)\n",
    "bestNet.test(arrtest)\n",
    "print bestNet.scores\n",
    "print bestNet.mainScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestNet.test(arrtrain)\n",
    "print bestNet.scores\n",
    "print bestNet.mainScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1\n",
      "Step: 2\n",
      "Step: 3\n",
      "Step: 4\n",
      "Step: 5\n",
      "Step: 6\n",
      "Step: 7\n",
      "Step: 8\n",
      "Step: 9\n",
      "Step: 10\n",
      "0.278160147725\n",
      "0.303775902373\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAD7CAYAAACYLnSTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrVD7Ve87YFgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f55dc26b690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "e = NeuralNetwork(640,10,[300,120,30],0.00001)\n",
    "e.train(arrtrain,arrtest)\n",
    "#print e.scores\n",
    "print e.mainScore\n",
    "e.test(arrtrain)\n",
    "print e.mainScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.385009232835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f55ddb6d510>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAD7CAYAAAAsJIKcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f55d6ec5090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "e.test(arrtest)\n",
    "print e.mainScore\n",
    "from sklearn.preprocessing import normalize\n",
    "normscores =normalize(e.scores)\n",
    "plt.imshow(normscores)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// OLD VERSION BELOW //\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 2.4 Build inference graph.\n",
    "def mnist_inference(sounds, hidden1_units, hidden2_units, hidden3_units):\n",
    "    \"\"\"Build the MNIST model up to where it may be used for inference.\n",
    "    Args:\n",
    "        images: Images placeholder.\n",
    "        hidden1_units: Size of the first hidden layer.\n",
    "        hidden2_units: Size of the second hidden layer.\n",
    "    Returns:\n",
    "        logits: Output tensor with the computed logits.\n",
    "    \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([1280, hidden1_units],\n",
    "                                stddev=1.0 / math.sqrt(float(1280))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                             name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(sounds, weights) + biases)\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                             name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "        \n",
    "    # Hidden 3\n",
    "    with tf.name_scope('hidden3'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden2_units, hidden3_units],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden3_units]),\n",
    "                             name='biases')\n",
    "        hidden3 = tf.nn.relu(tf.matmul(hidden2, weights) + biases)\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden3_units, 10],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden3_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([10]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden3, weights) + biases\n",
    "\n",
    "    # Uncomment the following line to see what we have constructed.\n",
    "    # tf.train.write_graph(tf.get_default_graph().as_graph_def(),\n",
    "    #                      \"/tmp\", \"inference.pbtxt\", as_text=True)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.5 Build training graph.\n",
    "def mnist_training(logits, labels, learning_rate):\n",
    "    \"\"\"Build the training graph.\n",
    "\n",
    "    Args:\n",
    "        logits: Logits tensor, float - [BATCH_SIZE, NUM_CLASSES].\n",
    "        labels: Labels tensor, int32 - [BATCH_SIZE], with values in the\n",
    "          range [0, NUM_CLASSES).\n",
    "        learning_rate: The learning rate to use for gradient descent.\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "        loss: The Op for calculating loss.\n",
    "    \"\"\"\n",
    "    # Create an operation that calculates loss.\n",
    "    labels = tf.to_int64(labels)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits, labels, name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Uncomment the following line to see what we have constructed.\n",
    "    # tf.train.write_graph(tf.get_default_graph().as_graph_def(),\n",
    "    #                      \"/tmp\", \"train.pbtxt\", as_text=True)\n",
    "\n",
    "    return train_op, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2.6 Build the complete graph for feeding inputs, training, and saving checkpoints.\n",
    "mnist_graph = tf.Graph()\n",
    "with mnist_graph.as_default():\n",
    "    # Generate placeholders for the images and labels.\n",
    "    images_placeholder = tf.placeholder(tf.float32)                                       \n",
    "    labels_placeholder = tf.placeholder(tf.int32)\n",
    "    tf.add_to_collection(\"images\", images_placeholder)  # Remember this Op.\n",
    "    tf.add_to_collection(\"labels\", labels_placeholder)  # Remember this Op.\n",
    "\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    logits = mnist_inference(images_placeholder,\n",
    "                             1000,\n",
    "                             600, \n",
    "                             170)\n",
    "    tf.add_to_collection(\"logits\", logits)  # Remember this Op.\n",
    "\n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    train_op, loss = mnist_training(logits, labels_placeholder, 0.0001)\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Uncomment the following line to see what we have constructed.\n",
    "    # tf.train.write_graph(tf.get_default_graph().as_graph_def(),\n",
    "    #                      \"/tmp\", \"complete.pbtxt\", as_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2.7 Run training for MAX_STEPS and save checkpoint at the end.\n",
    "with tf.Session(graph=mnist_graph) as sess:\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "    losses = []\n",
    "    counter=0\n",
    "    # Start the training loop.\n",
    "    for x in arrtest:\n",
    "        # Read a batch of images and labels.\n",
    "        #images_feed, labels_feed = data_sets.train.next_batch(BATCH_SIZE)\n",
    "        mel_feed, labels_feed = [],[]\n",
    "        for z in xrange(x.mel.shape[1]/10):\n",
    "            mel_feed.append(x.mel[:,z*10:(z+1)*10].flatten())\n",
    "            labels_feed.append(x.classNumber)\n",
    "        #print np.array(mel_feed).shape, \" \", np.array(labels_feed).shape\n",
    "        \n",
    "        \n",
    "        # Run one step of the model.  The return values are the activations\n",
    "        # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "        # inspect the values of your Ops or variables, you may include them\n",
    "        # in the list passed to sess.run() and the value tensors will be\n",
    "        # returned in the tuple from the call.\n",
    "        if not (len(labels_feed)==0):\n",
    "            _, loss_value = sess.run([train_op, loss],\n",
    "                                     feed_dict={images_placeholder: np.array(mel_feed),\n",
    "                                                labels_placeholder: np.array(labels_feed)})\n",
    "            losses.append(loss_value)\n",
    "        # Print out loss value.\n",
    "        counter +=1\n",
    "        if counter % 1000 == 0:\n",
    "            print('Step %d: loss = %.2f' % (counter, loss_value))\n",
    "                      \n",
    "    \n",
    "            \n",
    "    # Write a checkpoint.\n",
    "    #checkpoint_file = os.path.join(TRAIN_DIR, 'checkpoint')\n",
    "    #saver.save(sess, checkpoint_file, global_step=step)\n",
    "    checkpoint_file = os.path.join(\"/tmp/sound\", 'checkpoint')\n",
    "    saver.save(sess, checkpoint_file, global_step=counter)\n",
    "    plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluation = []\n",
    "for file in glob.glob(\"fold9/*.wav\"):\n",
    "    evaluation.append(Sound(file))\n",
    "for file in glob.glob(\"fold10/*.wav\"):\n",
    "    evaluation.append(Sound(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    saver = tf.train.import_meta_graph(\n",
    "        os.path.join(\"/tmp/sound\", \"checkpoint-7079.meta\"))\n",
    "    saver.restore(\n",
    "        sess, os.path.join(\"/tmp/sound\", \"checkpoint-7079\"))\n",
    "\n",
    "\n",
    "    # Retrieve the Ops we 'remembered'.\n",
    "    logits = tf.get_collection(\"logits\")[0]\n",
    "    images_placeholder = tf.get_collection(\"images\")[0]\n",
    "    labels_placeholder = tf.get_collection(\"labels\")[0]\n",
    "    # Add an Op that chooses the top k predictions.\n",
    "        \n",
    "    eval_op = tf.nn.top_k(logits)\n",
    "    \n",
    "    scores = np.zeros((10,10))\n",
    "    for x in evaluation:\n",
    "        good, total = 0, 0\n",
    "        temp = []\n",
    "        for z in xrange(x.mel.shape[1]/10):\n",
    "            mel_feed, labels_feed = [],[]\n",
    "            mel_feed.append(x.mel[:,z*10:(z+1)*10].flatten())\n",
    "            labels_feed.append(x.classNumber)\n",
    "            mel_feed, labels_feed = np.array(mel_feed), np.array(labels_feed)\n",
    "\n",
    "            # Run evaluation.\n",
    "            #images_feed, labels_feed = data_sets.validation.next_batch(EVAL_BATCH_SIZE)\n",
    "            prediction = sess.run(eval_op,\n",
    "                                  feed_dict={images_placeholder: mel_feed,\n",
    "                                             labels_placeholder: labels_feed})\n",
    "            #print type(labels_feed[0]), type(prediction.indices[0][0])\n",
    "            temp.append(prediction.indices[0][0])\n",
    "            #if labels_feed[0] == prediction.indices[0][0]:\n",
    "            #   good+=1\n",
    "            #total +=1\n",
    "        if temp:\n",
    "            scores[labels_feed[0],max(set(temp), key=temp.count)]+=1\n",
    "        #if good*2 >= total:\n",
    "         #   print labels_feed, \" : \", good, \"/\", total\n",
    "        #print count\n",
    "    print scores\n",
    "    counter = 0\n",
    "    for x in scores:\n",
    "        print \"Class %d: %.2f  Quantity: %d\" %(counter, x[counter]/np.sum(x), np.sum(x))\n",
    "        counter +=1\n",
    "    print \"Final score: \", scores.trace()/scores.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "normscores =normalize(scores)\n",
    "plt.imshow(normscores)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
