{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## used for folders/files searching and path finding\n",
    "import glob\n",
    "import os\n",
    "\n",
    "## sound preparation, feature extraction, preprocessing\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "## math and charts\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import specgram\n",
    "import math\n",
    "\n",
    "## some time measurement\n",
    "import time\n",
    "\n",
    "## data read/write with hdf5 files\n",
    "import h5py\n",
    "\n",
    "## random lmao\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO:\n",
    "\n",
    "RCNN\n",
    "DELETE SOME CODE\n",
    "ADD VISUALIZATION METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hop 10ms, window 40\n",
    "## OUTDATED, LOOK APPENDTRACKS FOR ITS REPLACEMENT\n",
    "## TO BE DELETED\n",
    "def appendSounds(data,labels,fp):\n",
    "    X, sr = sf.read(fp)\n",
    "    sound = np.array(X)\n",
    "    # librosa operates on (lenght, channels) matrices, wheras soundfile gave us (channels, lenght) \n",
    "    # so we transpose\n",
    "    sound = np.transpose(sound)\n",
    "    sound = librosa.core.to_mono(sound)\n",
    "    # resample so every wave has same sampling rate\n",
    "    sound = librosa.core.resample(sound, sr, 10000)\n",
    "    # set class number\n",
    "    classNumber = int(fp.split(\"/\")[1].split(\"-\")[1])\n",
    "    # compute and set mel spectrogram\n",
    "    D = np.abs(librosa.stft(sound, hop_length=506,  win_length=1024))**2\n",
    "    mel = librosa.logamplitude(librosa.feature.melspectrogram(S=D, sr=10000, n_mels = 64),ref_power= np.max)\n",
    "    iterator = 0\n",
    "    while iterator+16<=mel.shape[1]:\n",
    "        data.append(mel[:,iterator:iterator+16].flatten())\n",
    "        labels.append(classNumber)\n",
    "        iterator+=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hop 10ms, window 40\n",
    "def appendTracks(data,labels,fp):\n",
    "    X, sr = sf.read(fp)\n",
    "    sound = np.array(X)\n",
    "    # librosa operates on (lenght, channels) matrices, wheras soundfile gave us (channels, lenght) \n",
    "    # so we transpose\n",
    "    sound = np.transpose(sound)\n",
    "    sound = librosa.core.to_mono(sound)\n",
    "    # resample so every wave has same sampling rate\n",
    "    sound = librosa.core.resample(sound, sr, 10000)\n",
    "    # set class number\n",
    "    classNumber = int(fp.split(\"/\")[1].split(\"-\")[1])\n",
    "    # compute and set mel spectrogram\n",
    "    D = np.abs(librosa.stft(sound, hop_length=506,  win_length=1024))**2\n",
    "    mel = librosa.logamplitude(librosa.feature.melspectrogram(S=D, sr=10000, n_mels = 64),ref_power= np.max)\n",
    "    data.append(mel)\n",
    "    labels.append(classNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm done, time was: 389.736155987\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Lock, Pipe,Event\n",
    "\n",
    "## DO NOT USE IF HDF5 FILE HAS BEEN ALREADY CREATED!!!\n",
    "\n",
    "## whole thing is init method which preprocesses data and puts it into hdf5 file\n",
    "## only mel scaled spectrograms and classes of sound clips are preserved\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "## per process method, extracts data from given fold and sends it to main process\n",
    "def add(x,c):\n",
    "    tempdata,templabels = [],[]\n",
    "    for file in glob.glob(x):\n",
    "        appendTracks(tempdata, templabels, file)\n",
    "            \n",
    "    c.send(zip(tempdata, templabels))\n",
    "    del tempdata,templabels\n",
    "        \n",
    "\n",
    "ts = time.time()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    threads= []\n",
    "    connections=[0]*10\n",
    "    \n",
    "    \n",
    "    ## create threads, assign them methods and start \n",
    "    for x in xrange(1,11):\n",
    "        connections[x-1], childPipe=Pipe()\n",
    "        threads.append(Process(target=add, args=(\"fold\"+str(x)+\"/*.wav\",childPipe)))         \n",
    "        threads[x-1].start()\n",
    "    \n",
    "    ## wait for threads to finish, get data\n",
    "    for x,y in zip(threads,connections):\n",
    "        tD, tL = zip(*y.recv())\n",
    "        data.append((tD,tL))\n",
    "\n",
    "\n",
    "    \n",
    "## create hdf5 file and fill it with data\n",
    "## format is wholeTracks/fold[NUMBER]/[trackID] and keeps info about spectrogram, as well as class number\n",
    "## in attributes of dataset\n",
    "\n",
    "with h5py.File(\"wholeTracks.hdf5\") as f:\n",
    "    fold = 1\n",
    "    \n",
    "    for x in data:        \n",
    "        \n",
    "        grp = f.create_group(\"fold\"+str(fold))\n",
    "            \n",
    "        index = 0\n",
    "        tracks, cls = (x)\n",
    "        for track, cl in zip(tracks,cls):\n",
    "                \n",
    "            dset = grp.create_dataset(str(index),data=track)\n",
    "            dset.attrs['class']=cl\n",
    "            index +=1\n",
    "                \n",
    "        fold += 1        \n",
    "\n",
    "    f.close()\n",
    "    \n",
    "print \"I'm done, time was:\" , time.time()-ts\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splits data to train and test arrays, optional tune.\n",
    "# arrays are given in form of trainData, trainLabels...\n",
    "# method shall be fed with step between windows (equivalent to their width) and lists of folds that shall create \n",
    "# train, (tune), test arrays\n",
    "\n",
    "# train and tune arrays split tracks into windows of widht = spltStep, test array preserves whole track\n",
    "\n",
    "def split (splitStep, *data):\n",
    "    \n",
    "    with h5py.File(\"wholeTracks.hdf5\") as f:\n",
    "        \n",
    "        #no tune set\n",
    "        if len(data) == 2:\n",
    "            trainData, trainLabels, testData, testLabels = [],[],[],[]\n",
    "            \n",
    "            #create train arrays\n",
    "            for fold in data[0]:\n",
    "                grp = f['fold' + str(fold)]\n",
    "                \n",
    "                for track in grp.keys():\n",
    "                    \n",
    "                    tempTrack = grp[track][()]\n",
    "                    classNumber = grp[track].attrs['class']\n",
    "                    \n",
    "                    iterator = 0\n",
    "                    while iterator + splitStep <= tempTrack.shape[1]:\n",
    "                        trainData.append(tempTrack[: ,iterator : iterator + splitStep])\n",
    "                        trainLabels.append(classNumber)\n",
    "                        iterator += splitStep\n",
    "            \n",
    "            #create test arrays         \n",
    "            for fold in data[1]:\n",
    "                grp = f['fold' + str(fold)]\n",
    "                \n",
    "                for track in grp.keys():\n",
    "                    \n",
    "                    testData.append(grp[track][()])\n",
    "                    testLabels.append(grp[track].attrs['class'])\n",
    "                    \n",
    "            return trainData, trainLabels, testData, testLabels\n",
    "                 \n",
    "        # with tune set\n",
    "        elif len(data) == 3:\n",
    "            trainData, trainLabels, tuneData, tuneLabels, testData, testLabels = [],[],[],[],[],[]\n",
    "            \n",
    "            # create train arrays\n",
    "            for fold in data[0]:\n",
    "                grp = f['fold' + str(fold)]\n",
    "                \n",
    "                for track in grp.keys():\n",
    "                    \n",
    "                    tempTrack = grp[track][()]\n",
    "                    classNumber = grp[track].attrs['class']\n",
    "                    \n",
    "                    iterator = 0\n",
    "                    while iterator + splitStep <= tempTrack.shape[1]:\n",
    "                        trainData.append(tempTrack[: ,iterator : iterator + splitStep])\n",
    "                        trainLabels.append(classNumber)\n",
    "                        iterator += splitStep\n",
    "                        \n",
    "            # create tune arrays\n",
    "            for fold in data[1]:\n",
    "                grp = f['fold' + str(fold)]\n",
    "                \n",
    "                for track in grp.keys():\n",
    "                    \n",
    "                    tempTrack = grp[track][()]\n",
    "                    classNumber = grp[track].attrs['class']\n",
    "                    \n",
    "                    iterator = 0\n",
    "                    while iterator + splitStep <= tempTrack.shape[1]:\n",
    "                        tuneData.append(tempTrack[: ,iterator : iterator + splitStep])\n",
    "                        tuneLabels.append(classNumber)\n",
    "                        iterator += splitStep\n",
    "            \n",
    "            # create test arrays\n",
    "            for fold in data[2]:\n",
    "                grp = f['fold' + str(fold)]\n",
    "                \n",
    "                for track in grp.keys():\n",
    "                    \n",
    "                    testData.append(grp[track][()])\n",
    "                    testLabels.append(grp[track].attrs['class'])\n",
    "            \n",
    "            return trainData, trainLabels, tuneData, tuneLabels, testData, testLabels\n",
    "\n",
    "        else:\n",
    "            raise NameError(\"Wrong number of inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## predicts classes on whole database or specified fold from database\n",
    "def predictOnDataBase(model, splitStep, *data):\n",
    "    with h5py.File(\"wholeTracks.hdf5\") as f:\n",
    "        ## prepare list of folds to work on\n",
    "        if len(data) == 0:\n",
    "            grps = []\n",
    "            for grp in f:\n",
    "                grps.append(f[grp])\n",
    "        else:\n",
    "            if type(data[0]) is list:\n",
    "                for fold in data[0]:\n",
    "                    grps.append(f['fold' + str(fold)])\n",
    "            else:\n",
    "                for fold in data:\n",
    "                    grps.append(f['fold' + str(fold)])\n",
    "        \n",
    "        ## assign scores to every file in fold        \n",
    "        for grp in grps:\n",
    "            for clip in grp.keys():\n",
    "                clip = grp[clip]\n",
    "                if 'predictions' in clip.attrs:\n",
    "                    del clip.attrs['predictions']\n",
    "                \n",
    "                temp = clip[()]\n",
    "                trackToPredict = []\n",
    "                iterator = 0\n",
    "                    \n",
    "                ## if clip is too short to predict even 1 window, expand it\n",
    "                if temp.shape[1]<16:\n",
    "                    t = temp.shape[1]\n",
    "                    b = np.zeros((64,16))\n",
    "                    b[:, :-16+t]= temp\n",
    "                    trackToPredict.append(b)\n",
    "                else:\n",
    "                    ## iterate through clip and split it into windows\n",
    "                    while iterator + splitStep <= temp.shape[1]:\n",
    "                        trackToPredict.append(temp[: ,iterator : iterator + splitStep])\n",
    "                        iterator += splitStep\n",
    "                    \n",
    "                ## prepare clip as a corrent CNN input and predict probabilities\n",
    "                trackToPredict= np.array(trackToPredict)\n",
    "                trackToPredict = np.expand_dims(trackToPredict, axis=3)\n",
    "                predicted = model.predict(trackToPredict, batch_size=1, verbose=2)\n",
    "\n",
    "                ## predicted is a matrix of probabilities, where each row has 10 probabilities of being\n",
    "                ## specific class, each row represents one predicted window\n",
    "                \n",
    "                ## to predictions append list of best prediction of class for each window\n",
    "                ## predicted is majority vote over all windows (but voted using probabilities, not classes)\n",
    "                clip.attrs['predictions']=np.argmax(predicted,axis=1)\n",
    "                clip.attrs['predicted']=predicted.sum(axis=0).argmax()\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## prints score and accuracy matrix for whole dataset or for given folds\n",
    "def getScores(*data):\n",
    "    scores = np.zeros((10,10))\n",
    "    with h5py.File(\"wholeTracks.hdf5\") as f:\n",
    "        grps = []\n",
    "        ## prepare folds to print scores for\n",
    "        if not data:\n",
    "            for grp in f:\n",
    "                grps.append(f[grp])\n",
    "        else:\n",
    "            if type(data[0]) is list:\n",
    "                for fold in data[0]:\n",
    "                    grps.append(f['fold' + str(fold)])\n",
    "            else:\n",
    "                for fold in data:\n",
    "                    grps.append(f['fold' + str(fold)])\n",
    "\n",
    "        for grp in grps:\n",
    "            for clip in grp.keys():\n",
    "                clip = grp[clip]\n",
    "\n",
    "                scores[clip.attrs['predicted'],clip.attrs['class']] += 1\n",
    "                \n",
    "                \n",
    "            \n",
    "    ## returns confusion matrix (where rows are predicted classes and columns are actuall classes)\n",
    "    ## as well as accuracy \n",
    "    return scores, scores.trace()/scores.sum()\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look for clips searching for class or/and prediction\n",
    "def findClips(actual = None, predicted = None):\n",
    "    if actual == None and predicted == None:\n",
    "        print \"Specify actual or predicted classes to search for\"\n",
    "        return None\n",
    "    if actual:\n",
    "        if type(actual) is not list:\n",
    "            temp = actual\n",
    "            actual = []\n",
    "            actual.append(temp)\n",
    "        actual = set(actual)\n",
    "    if predicted:\n",
    "        if type(predicted) is not list:\n",
    "            temp = predicted\n",
    "            predicted = []\n",
    "            predicted.append(temp)\n",
    "        predicted = set(predicted)\n",
    "    \n",
    "    toReturn =[]\n",
    "    \n",
    "    with h5py.File(\"wholeTracks.hdf5\") as f:\n",
    "        \n",
    "        if actual and predicted:\n",
    "            for fold in f:\n",
    "                fold = f[fold]\n",
    "                for clip in fold:\n",
    "                    clip = fold[clip]\n",
    "                    if clip.attrs['class'] in actual and clip.attrs['predicted'] in predicted:\n",
    "                        toReturn.append(clip.name)\n",
    "                    \n",
    "        elif actual:\n",
    "            for fold in f:\n",
    "                fold = f[fold]\n",
    "                for clip in fold:\n",
    "                    clip = fold[clip]\n",
    "                    if clip.attrs['class'] in actual:\n",
    "                        toReturn.append(clip.name)\n",
    "                        \n",
    "        elif predicted:\n",
    "            for fold in f:\n",
    "                fold = f[fold]\n",
    "                for clip in fold:\n",
    "                    clip = fold[clip]\n",
    "                    if clip.attrs['predicted'] in predicted:\n",
    "                        toReturn.append(clip.name)\n",
    "    \n",
    "    return toReturn\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fold1/287\n",
      "/fold10/323\n"
     ]
    }
   ],
   "source": [
    "for x in findClips(actual=1,predicted = 5):\n",
    "    print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## OUTDATED - REPLACED BY HDF5\n",
    "## KEPT JUST IN CASE, TO BE DELETED\n",
    "\n",
    "from multiprocessing import Process, Lock, Pipe,Event\n",
    "import time\n",
    "from keras import utils\n",
    "\n",
    "\n",
    "allData,allLabels=[],[]\n",
    "trainData,trainLabels= [],[]\n",
    "tuneData,tuneLabels=[],[]\n",
    "testData,testLabels=[],[]\n",
    "#function to extract data using multiprocessing\n",
    "def add(x,c):\n",
    "    tempdata,templabels = [],[]\n",
    "    if int(x[4])<9:\n",
    "        try:\n",
    "            int(x[5])\n",
    "        except:\n",
    "            #split for training set - loses information about which clip batch comes from \n",
    "            for file in glob.glob(x):\n",
    "                appendSounds(tempdata, templabels, file)\n",
    "        else:\n",
    "            #split for testing set - presumes batches of sound within clip\n",
    "            for file in glob.glob(x):\n",
    "                appendTracks(tempdata, templabels, file)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #split for testing set - presumes batches of sound within clip\n",
    "        for file in glob.glob(x):\n",
    "            appendTracks(tempdata, templabels, file)\n",
    "            \n",
    "    c.send(zip(tempdata, templabels))\n",
    "    del tempdata,templabels\n",
    "        \n",
    "ts = time.time()\n",
    "\n",
    "#doing actual multiprocessing extraction\n",
    "if __name__ == '__main__':\n",
    "    threads= []\n",
    "    connections=[0]*10\n",
    "    lock = Lock()\n",
    "    for x in xrange(1,11):\n",
    "        connections[x-1], childPipe=Pipe()\n",
    "        threads.append(Process(target=add, args=(\"fold\"+str(x)+\"/*.wav\",childPipe)))         \n",
    "        threads[x-1].start()\n",
    "    fold = 1\n",
    "    for x,y in zip(threads,connections):\n",
    "        tD, tL = zip(*y.recv())\n",
    "        if fold <= 6:\n",
    "            trainData += tD\n",
    "            trainLabels += tL\n",
    "        elif fold <= 8 :\n",
    "            tuneData += tD\n",
    "            tuneLabels += tL\n",
    "        else:\n",
    "            testData += tD\n",
    "            testLabels += tL\n",
    "        fold+=1\n",
    "        \n",
    "    #save data into numpy array, change classes label using onehot encoding\n",
    "    trainData, trainLabels = np.array(trainData), np.array(utils.to_categorical(trainLabels, num_classes=10))\n",
    "    tuneData, tuneLabels = np.array(tuneData), np.array(utils.to_categorical(tuneLabels, num_classes=10))\n",
    "    #testData, testLabels = np.array(testData), np.array(utils.to_categorical(testLabels, num_classes=10))\n",
    "    \n",
    "    print \"I'm done, time was:\" , time.time()-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7322/7322 [==============================] - 0s\n",
      "23987/23987 [==============================] - 0s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Tune score: 0.294 Train score: 0.351 Learning rate: 0.008000 L2 reg: 0.002070\n"
     ]
    }
   ],
   "source": [
    "## NO PROPER DATA PREPARATION FOR THIS PART RIGHT NOW, GIVES POOR SCORES ANYWAY\n",
    "## PROCEED TO CNN PART\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from random import uniform\n",
    "from math import pow\n",
    "rates2 = []\n",
    "\n",
    "#standard NN - was fine tuned for another example, outdated right now\n",
    "\n",
    "LR,L2= 0.008,0.00207\n",
    "net = Sequential()\n",
    "net.add(Dense(512, activation='relu', input_dim=1024,\n",
    "             kernel_regularizer=regularizers.l2(L2)))\n",
    "net.add(Dropout(0.5))\n",
    "net.add(Dense(256, activation='relu',\n",
    "             kernel_regularizer=regularizers.l2(L2)))\n",
    "net.add(Dropout(0.3))\n",
    "net.add(Dense(64, activation='relu',\n",
    "             kernel_regularizer=regularizers.l2(L2)))\n",
    "net.add(Dropout(0.1))\n",
    "net.add(Dense(10, activation='softmax'))\n",
    "net.compile(loss='categorical_crossentropy',\n",
    "           optimizer = SGD(lr=LR,decay=1e-6, momentum=0.9,nesterov=True),\n",
    "           metrics = ['accuracy'])\n",
    "\n",
    "net.fit(trainData, trainLabels, epochs = 100, batch_size=128, verbose=0)\n",
    "rates2.append([net.evaluate(tuneData,tuneLabels, batch_size=16384)[1],\n",
    "             net.evaluate(trainData,trainLabels, batch_size=16384)[1],\n",
    "             LR,L2])\n",
    "print (\"Tune score: %.3f Train score: %.3f Learning rate: %f L2 reg: %f\" %(rates2[-1][0],rates2[-1][1],LR, L2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## prepare data for CNN training\n",
    "\n",
    "## split folds into train, tune and test sets\n",
    "trainData, trainLabels, tuneData, tuneLabels, testData, testLabels = split(16, [1,2,3,4,5,6,7],[8],[9,10])\n",
    "\n",
    "## lists to np arrays\n",
    "trainData=np.array(trainData)\n",
    "trainLabels=np.array(trainLabels)\n",
    "tuneData=np.array(tuneData)\n",
    "tuneLabels=np.array(tuneLabels)\n",
    "\n",
    "\n",
    "## add 3rd dim - channel required by conv layers\n",
    "trainData=np.expand_dims(trainData,axis=3)\n",
    "tuneData=np.expand_dims(tuneData,axis=3)\n",
    "\n",
    "from keras import utils\n",
    "\n",
    "## classes to vector - one hot encode\n",
    "trainLabels = np.array(utils.to_categorical(trainLabels, num_classes=10))\n",
    "tuneLabels = np.array(utils.to_categorical(tuneLabels, num_classes=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5s - loss: 1.6157 - acc: 0.4540\n",
      "Epoch 2/100\n",
      "3s - loss: 1.0621 - acc: 0.6383\n",
      "Epoch 3/100\n",
      "3s - loss: 0.8567 - acc: 0.7188\n",
      "Epoch 4/100\n",
      "3s - loss: 0.7230 - acc: 0.7644\n",
      "Epoch 5/100\n",
      "3s - loss: 0.6257 - acc: 0.7959\n",
      "Epoch 6/100\n",
      "3s - loss: 0.5482 - acc: 0.8220\n",
      "Epoch 7/100\n",
      "3s - loss: 0.4948 - acc: 0.8405\n",
      "Epoch 8/100\n",
      "3s - loss: 0.4461 - acc: 0.8552\n",
      "Epoch 9/100\n",
      "3s - loss: 0.4114 - acc: 0.8642\n",
      "Epoch 10/100\n",
      "3s - loss: 0.3856 - acc: 0.8714\n",
      "Epoch 11/100\n",
      "3s - loss: 0.3533 - acc: 0.8822\n",
      "Epoch 12/100\n",
      "3s - loss: 0.3333 - acc: 0.8885\n",
      "Epoch 13/100\n",
      "3s - loss: 0.3156 - acc: 0.8944\n",
      "Epoch 14/100\n",
      "3s - loss: 0.2981 - acc: 0.9028\n",
      "Epoch 15/100\n",
      "3s - loss: 0.2758 - acc: 0.9052\n",
      "Epoch 16/100\n",
      "3s - loss: 0.2677 - acc: 0.9093\n",
      "Epoch 17/100\n",
      "3s - loss: 0.2525 - acc: 0.9138\n",
      "Epoch 18/100\n",
      "3s - loss: 0.2496 - acc: 0.9168\n",
      "Epoch 19/100\n",
      "3s - loss: 0.2294 - acc: 0.9224\n",
      "Epoch 20/100\n",
      "3s - loss: 0.2208 - acc: 0.9253\n",
      "Epoch 21/100\n",
      "3s - loss: 0.2145 - acc: 0.9277\n",
      "Epoch 22/100\n",
      "3s - loss: 0.2027 - acc: 0.9292\n",
      "Epoch 23/100\n",
      "3s - loss: 0.1932 - acc: 0.9348\n",
      "Epoch 24/100\n",
      "3s - loss: 0.1852 - acc: 0.9374\n",
      "Epoch 25/100\n",
      "3s - loss: 0.1767 - acc: 0.9399\n",
      "Epoch 26/100\n",
      "3s - loss: 0.1758 - acc: 0.9395\n",
      "Epoch 27/100\n",
      "3s - loss: 0.1663 - acc: 0.9438\n",
      "Epoch 28/100\n",
      "3s - loss: 0.1604 - acc: 0.9437\n",
      "Epoch 29/100\n",
      "3s - loss: 0.1533 - acc: 0.9472\n",
      "Epoch 30/100\n",
      "3s - loss: 0.1508 - acc: 0.9480\n",
      "Epoch 31/100\n",
      "3s - loss: 0.1425 - acc: 0.9503\n",
      "Epoch 32/100\n",
      "3s - loss: 0.1373 - acc: 0.9525\n",
      "Epoch 33/100\n",
      "3s - loss: 0.1327 - acc: 0.9539\n",
      "Epoch 34/100\n",
      "3s - loss: 0.1325 - acc: 0.9532\n",
      "Epoch 35/100\n",
      "3s - loss: 0.1304 - acc: 0.9558\n",
      "Epoch 36/100\n",
      "3s - loss: 0.1296 - acc: 0.9539\n",
      "Epoch 37/100\n",
      "3s - loss: 0.1212 - acc: 0.9571\n",
      "Epoch 38/100\n",
      "3s - loss: 0.1177 - acc: 0.9589\n",
      "Epoch 39/100\n",
      "3s - loss: 0.1168 - acc: 0.9596\n",
      "Epoch 40/100\n",
      "3s - loss: 0.1078 - acc: 0.9625\n",
      "Epoch 41/100\n",
      "3s - loss: 0.1112 - acc: 0.9597\n",
      "Epoch 42/100\n",
      "3s - loss: 0.1087 - acc: 0.9616\n",
      "Epoch 43/100\n",
      "3s - loss: 0.1017 - acc: 0.9638\n",
      "Epoch 44/100\n",
      "3s - loss: 0.1001 - acc: 0.9649\n",
      "Epoch 45/100\n",
      "3s - loss: 0.0953 - acc: 0.9660\n",
      "Epoch 46/100\n",
      "3s - loss: 0.0890 - acc: 0.9679\n",
      "Epoch 47/100\n",
      "3s - loss: 0.0908 - acc: 0.9692\n",
      "Epoch 48/100\n",
      "3s - loss: 0.0940 - acc: 0.9668\n",
      "Epoch 49/100\n",
      "3s - loss: 0.0909 - acc: 0.9677\n",
      "Epoch 50/100\n",
      "3s - loss: 0.0868 - acc: 0.9695\n",
      "Epoch 51/100\n",
      "3s - loss: 0.0867 - acc: 0.9692\n",
      "Epoch 52/100\n",
      "3s - loss: 0.0794 - acc: 0.9723\n",
      "Epoch 53/100\n",
      "3s - loss: 0.0800 - acc: 0.9725\n",
      "Epoch 54/100\n",
      "3s - loss: 0.0820 - acc: 0.9711\n",
      "Epoch 55/100\n",
      "3s - loss: 0.0807 - acc: 0.9710\n",
      "Epoch 56/100\n",
      "3s - loss: 0.0768 - acc: 0.9723\n",
      "Epoch 57/100\n",
      "3s - loss: 0.0753 - acc: 0.9743\n",
      "Epoch 58/100\n",
      "3s - loss: 0.0758 - acc: 0.9737\n",
      "Epoch 59/100\n",
      "3s - loss: 0.0762 - acc: 0.9739\n",
      "Epoch 60/100\n",
      "3s - loss: 0.0727 - acc: 0.9749\n",
      "Epoch 61/100\n",
      "3s - loss: 0.0705 - acc: 0.9757\n",
      "Epoch 62/100\n",
      "3s - loss: 0.0667 - acc: 0.9777\n",
      "Epoch 63/100\n",
      "3s - loss: 0.0706 - acc: 0.9747\n",
      "Epoch 64/100\n",
      "3s - loss: 0.0652 - acc: 0.9770\n",
      "Epoch 65/100\n",
      "3s - loss: 0.0656 - acc: 0.9769\n",
      "Epoch 66/100\n",
      "3s - loss: 0.0667 - acc: 0.9778\n",
      "Epoch 67/100\n",
      "3s - loss: 0.0630 - acc: 0.9776\n",
      "Epoch 68/100\n",
      "3s - loss: 0.0642 - acc: 0.9778\n",
      "Epoch 69/100\n",
      "3s - loss: 0.0626 - acc: 0.9781\n",
      "Epoch 70/100\n",
      "3s - loss: 0.0604 - acc: 0.9778\n",
      "Epoch 71/100\n",
      "3s - loss: 0.0566 - acc: 0.9803\n",
      "Epoch 72/100\n",
      "3s - loss: 0.0563 - acc: 0.9807\n",
      "Epoch 73/100\n",
      "3s - loss: 0.0597 - acc: 0.9780\n",
      "Epoch 74/100\n",
      "3s - loss: 0.0562 - acc: 0.9809\n",
      "Epoch 75/100\n",
      "3s - loss: 0.0551 - acc: 0.9807\n",
      "Epoch 76/100\n",
      "3s - loss: 0.0531 - acc: 0.9814\n",
      "Epoch 77/100\n",
      "3s - loss: 0.0548 - acc: 0.9807\n",
      "Epoch 78/100\n",
      "3s - loss: 0.0532 - acc: 0.9809\n",
      "Epoch 79/100\n",
      "3s - loss: 0.0514 - acc: 0.9819\n",
      "Epoch 80/100\n",
      "3s - loss: 0.0532 - acc: 0.9815\n",
      "Epoch 81/100\n",
      "3s - loss: 0.0503 - acc: 0.9817\n",
      "Epoch 82/100\n",
      "3s - loss: 0.0498 - acc: 0.9822\n",
      "Epoch 83/100\n",
      "3s - loss: 0.0482 - acc: 0.9834\n",
      "Epoch 84/100\n",
      "3s - loss: 0.0497 - acc: 0.9826\n",
      "Epoch 85/100\n",
      "3s - loss: 0.0487 - acc: 0.9833\n",
      "Epoch 86/100\n",
      "3s - loss: 0.0497 - acc: 0.9825\n",
      "Epoch 87/100\n",
      "3s - loss: 0.0514 - acc: 0.9823\n",
      "Epoch 88/100\n",
      "3s - loss: 0.0494 - acc: 0.9831\n",
      "Epoch 89/100\n",
      "3s - loss: 0.0462 - acc: 0.9834\n",
      "Epoch 90/100\n",
      "3s - loss: 0.0485 - acc: 0.9820\n",
      "Epoch 91/100\n",
      "3s - loss: 0.0439 - acc: 0.9843\n",
      "Epoch 92/100\n",
      "3s - loss: 0.0436 - acc: 0.9849\n",
      "Epoch 93/100\n",
      "3s - loss: 0.0428 - acc: 0.9856\n",
      "Epoch 94/100\n",
      "3s - loss: 0.0431 - acc: 0.9847\n",
      "Epoch 95/100\n",
      "3s - loss: 0.0404 - acc: 0.9860\n",
      "Epoch 96/100\n",
      "3s - loss: 0.0435 - acc: 0.9843\n",
      "Epoch 97/100\n",
      "3s - loss: 0.0408 - acc: 0.9859\n",
      "Epoch 98/100\n",
      "3s - loss: 0.0395 - acc: 0.9860\n",
      "Epoch 99/100\n",
      "3s - loss: 0.0391 - acc: 0.9862\n",
      "Epoch 100/100\n",
      "3s - loss: 0.0402 - acc: 0.9861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f217c450a50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,ZeroPadding2D,BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "#CNN working on batches of image, accuracy up to 72%\n",
    "\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D(padding = (1,1), input_shape=(64, 16, 1)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(ZeroPadding2D(padding = (1,1)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(ZeroPadding2D(padding = (1,1)))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics = ['accuracy'])\n",
    "\n",
    "model.fit(trainData, trainLabels, batch_size=512, epochs=100,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67332123412\n"
     ]
    }
   ],
   "source": [
    "## AS WELL OUTDATED, SCORES ARE NOW BASED ON PREDICTIONS SAVED IN HDF5 FILE\n",
    "\n",
    "#final scoring function. isnt used in training.\n",
    "#CNN predicts classes for all parts of clip and sums probablities, chooses best\n",
    "#scores is confusion matrix\n",
    "scores = np.zeros((10,10))\n",
    "for track,label in zip(testData,testLabels):\n",
    "    tempD =[]\n",
    "    iterator = 0\n",
    "    ## if track is too short for even one window to predict, fill it up with zeros to one window size\n",
    "    if track.shape[1]<16:\n",
    "        t = track.shape[1]\n",
    "        b = np.zeros((64,16))\n",
    "        b[:, :-16+t]= track\n",
    "        tempD.append(b)\n",
    "    while (iterator+16<=track.shape[1]):\n",
    "        tempD.append(track[:,iterator:iterator+16])\n",
    "        iterator+=16\n",
    "    tempD= np.array(tempD)\n",
    "    tempD = np.expand_dims(tempD, axis=3)\n",
    "    predicted = model.predict(tempD, batch_size=1, verbose=2)\n",
    "    scores[predicted.sum(axis=0).argmax(),label]+=1\n",
    "\n",
    "accuracy = scores.trace()/scores.sum()    \n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 109.    0.    0.    4.    3.   27.    1.   25.    0.    0.]\n",
      " [   3.   63.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
      " [  93.    6.  274.   46.   28.   55.    1.   28.   32.   55.]\n",
      " [  13.    4.   17.  224.   19.    2.   26.    2.   10.    7.]\n",
      " [  19.    1.    1.    5.  196.    0.    1.   67.   10.    0.]\n",
      " [   1.    1.    1.    1.    2.  151.    0.    5.    2.    1.]\n",
      " [  11.    1.    1.    0.    1.    0.   64.    0.    0.    1.]\n",
      " [   6.   12.    1.    3.   32.   17.    0.  126.    0.    0.]\n",
      " [  25.    0.    3.    9.    8.    5.    0.    1.  190.    8.]\n",
      " [  20.    7.    2.    8.   11.   13.    0.    2.    1.  228.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAAD8CAYAAAA11GIZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f217c4d4f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "## print scores and graphic version\n",
    "## confusion matrix, x - actual class, y - predicted class\n",
    "scores = getScores(8,9,10)[0]\n",
    "print scores\n",
    "normscores =normalize(scores,norm='l1' ,axis=0)\n",
    "plt.imshow(normscores)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some charts used in finetuning of previous NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD7CAYAAABqvuNzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3599b7eed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rates[:,2],rates[:,0],'ro')\n",
    "plt.axis([0.001,0.01,0.35,0.5])\n",
    "plt.show()\n",
    "#128 batch - f(learning_Rate)=score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD7CAYAAABqvuNzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f352f5d63d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rates1[:,3],rates1[:,0],'ro')\n",
    "plt.axis([0.0001,0.01,0.4,0.5])\n",
    "plt.show()\n",
    "#128 batch - f(L2 lambda)=score, lr=0.0038"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
